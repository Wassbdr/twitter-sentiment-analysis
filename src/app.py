import streamlit as st
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Chargement du mod√®le fine-tun√©
@st.cache_resource
def load_model():
    model_path = "./tweet-model"
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    return tokenizer, model

tokenizer, model = load_model()

# G√©n√©ration d‚Äôun tweet √† partir du sentiment s√©lectionn√©
def generate_tweet_finetuned(sentiment="positive", max_length=50):
    prompt = f"<{sentiment}>"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        do_sample=True,
        top_p=0.95,
        top_k=50,
        temperature=0.8,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()

# Interface Streamlit
st.title("üß† G√©n√©rateur de Tweets Financiers")
st.markdown("Ce g√©n√©rateur utilise un mod√®le GPT-2 fine-tun√© pour produire des tweets financiers selon un sentiment donn√©.")

sentiment = st.selectbox("Choisissez un sentiment :", ["positive", "neutral", "negative"])

if st.button("G√©n√©rer un tweet"):
    with st.spinner("G√©n√©ration en cours..."):
        tweet = generate_tweet_finetuned(sentiment)
    st.success("Tweet g√©n√©r√© :")
    st.write(tweet)

